{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "\n",
    "\n",
    "from database_functions import (\n",
    "    get_config,\n",
    "    insert_values, \n",
    "    general_query)\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "TABLE_NAME_TARGET='crypto_timeseries'\n",
    "\n",
    "CONFIG_DB = get_config(filename=\"config.ini\", section=\"crypto\")\n",
    "CONFIG_API = get_config(filename=\"config.ini\", section=\"api\")\n",
    "CONFIG_SPARK= get_config(filename=\"config.ini\", section=\"apache-spark\")\n",
    "\n",
    "API_URL=\"https://rest.coincap.io/v3/assets?apiKey={api_key}\"\n",
    "HEADER_API = {\n",
    "            \"Content-Type\":\"application/json\",\n",
    "            \"Accept-Encoding\":\"deflate\" \n",
    "        }\n",
    "\n",
    "SPARK_SESSION_NAME=\"main_etl_pyspark\"\n",
    "POSTRESQL_JARFILE=CONFIG_SPARK['postresql_jar']\n",
    "COLUMN_DATATYPES = {\n",
    "            \"timestamp\":            \"timestamp\",\n",
    "            \"id\":                   \"string\",\n",
    "            \"rank\":                 \"int\",\n",
    "            \"symbol\":               \"string\",\n",
    "            \"name\":                 \"string\",\n",
    "            \"supply\":               \"double\",\n",
    "            \"maxsupply\":            \"double\",\n",
    "            \"marketcapusd\":         \"double\",\n",
    "            \"volumeusd24hr\":        \"double\",\n",
    "            \"priceusd\":             \"double\",\n",
    "            \"changepercent24hr\":    \"double\",\n",
    "            \"vwap24hr\":             \"double\",\n",
    "            \"explorer\":             \"string\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate spark session\n",
    "print(f\"\\nInit Spark session..\")\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(SPARK_SESSION_NAME) \\\n",
    "    .config(\"spark.jars\", POSTRESQL_JARFILE) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"\\nSpark is running at: \\n{spark._jsc.sc().uiWebUrl().get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> ETRACT <----- ##\n",
    "print(f\"\\nEXTRACT from API query..\")\n",
    "#api query\n",
    "response = requests.get(url=API_URL.format(api_key=CONFIG_API['api_key']), \n",
    "                        headers=HEADER_API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> TRANSFORM <----- ##\n",
    "print(f\"\\nTRANSFORM..\")\n",
    "#api json response to pandas Dataframe\n",
    "responseData=response.json()\n",
    "df = pd.json_normalize(data=responseData, record_path='data')\n",
    "\n",
    "#insert timestamp\n",
    "current_timestamp = datetime.now()\n",
    "current_timestamp.strftime('%d-%m-%Y %H:%M:%S')\n",
    "df['timestamp'] = [current_timestamp]*df.shape[0]\n",
    "\n",
    "#rename columns to lowercase\n",
    "rename_cols_dict={c:c.lower() for c in df.columns.tolist()}\n",
    "df.rename(columns=rename_cols_dict, inplace=True)\n",
    "\n",
    "#drop columns with tokens*\n",
    "df = df.loc[:, ~df.columns.str.startswith(\"tokens.\")]\n",
    "\n",
    "#pandas Dataframe --> spark DataFrame\n",
    "dfs = spark.createDataFrame(df)\n",
    "\n",
    "#assure expected columns data types\n",
    "for column_name, data_type in COLUMN_DATATYPES.items():\n",
    "    dfs = dfs\\\n",
    "        .withColumn(column_name, col(column_name).cast(data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> LOAD <----- ##\n",
    "print(f\"\\nLOAD..\")\n",
    "#connect Spark to to postgreSQL database\n",
    "url_db = f\"jdbc:postgresql://{CONFIG_DB['host']}:{CONFIG_DB['port']}/{CONFIG_DB['database']}\"\n",
    "properties_dbspark = {\n",
    "        \"user\":     f\"{CONFIG_DB['user']}\",\n",
    "        \"password\": f\"{CONFIG_DB['password']}\",\n",
    "        \"driver\":   \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "#load PySpark DataFrame to postresql database\n",
    "dfs.write.jdbc(\n",
    "                url         =   url_db, \n",
    "                table       =   TABLE_NAME_TARGET, \n",
    "                mode        =   \"append\", \n",
    "                properties  =   properties_dbspark\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
