{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "findspark.init()\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Include base directory\n",
    "notebook_dir = Path(os.getcwd())\n",
    "sys.path.append((notebook_dir.parent).as_posix())\n",
    "\n",
    "from database_functions import (\n",
    "    get_config,\n",
    "    insert_values, \n",
    "    general_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "TABLE_NAME_TARGET='crypto_timeseries'\n",
    "\n",
    "CONFIG_DB = get_config(filename=\"config.ini\", section=\"crypto\")\n",
    "CONFIG_API = get_config(filename=\"config.ini\", section=\"api\")\n",
    "CONFIG_SPARK= get_config(filename=\"config.ini\", section=\"apache-spark\")\n",
    "\n",
    "SPARK_SESSION_NAME=\"main_etl_pyspark\"\n",
    "COLUMN_DATATYPES = {\n",
    "            \"timestamp\":            \"timestamp\",\n",
    "            \"id\":                   \"string\",\n",
    "            \"rank\":                 \"int\",\n",
    "            \"symbol\":               \"string\",\n",
    "            \"name\":                 \"string\",\n",
    "            \"supply\":               \"double\",\n",
    "            \"maxsupply\":            \"double\",\n",
    "            \"marketcapusd\":         \"double\",\n",
    "            \"volumeusd24hr\":        \"double\",\n",
    "            \"priceusd\":             \"double\",\n",
    "            \"changepercent24hr\":    \"double\",\n",
    "            \"vwap24hr\":             \"double\",\n",
    "            \"explorer\":             \"string\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate spark session\n",
    "print(f\"\\nInit Spark session..\")\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(SPARK_SESSION_NAME) \\\n",
    "    .config(\"spark.jars\", CONFIG_SPARK['postresql_jar']) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"\\nSpark is running at: \\n{spark._jsc.sc().uiWebUrl().get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> ETRACT <----- ##\n",
    "print(f\"\\nEXTRACT from API query..\")\n",
    "response = requests.get(url=CONFIG_API['url'].format(api_key=CONFIG_API['api_key']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> TRANSFORM <----- ##\n",
    "print(f\"\\nTRANSFORM..\")\n",
    "#api json response to pandas Dataframe\n",
    "responseData=response.json()\n",
    "df = pd.json_normalize(data=responseData, record_path='data')\n",
    "\n",
    "#insert timestamp\n",
    "current_timestamp = datetime.now()\n",
    "current_timestamp.strftime('%d-%m-%Y %H:%M:%S')\n",
    "df['timestamp'] = [current_timestamp]*df.shape[0]\n",
    "\n",
    "#rename columns to lowercase\n",
    "rename_cols_dict={c:c.lower() for c in df.columns.tolist()}\n",
    "df.rename(columns=rename_cols_dict, inplace=True)\n",
    "\n",
    "#drop columns with tokens*\n",
    "df = df.loc[:, ~df.columns.str.startswith(\"tokens.\")]\n",
    "\n",
    "#pandas Dataframe --> spark DataFrame\n",
    "dfs = spark.createDataFrame(df)\n",
    "\n",
    "#assure expected columns data types\n",
    "for column_name, data_type in COLUMN_DATATYPES.items():\n",
    "    dfs = dfs\\\n",
    "        .withColumn(column_name, col(column_name).cast(data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##----> LOAD <----- ##\n",
    "print(f\"\\nLOAD..\")\n",
    "#connect Spark to to postgreSQL database\n",
    "url_db = f\"jdbc:postgresql://{CONFIG_DB['host']}:{CONFIG_DB['port']}/{CONFIG_DB['database']}\"\n",
    "properties_dbspark = {\n",
    "        \"user\":     f\"{CONFIG_DB['user']}\",\n",
    "        \"password\": f\"{CONFIG_DB['password']}\",\n",
    "        \"driver\":   \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "#load PySpark DataFrame to postresql database\n",
    "dfs.write.jdbc(\n",
    "                url         =   url_db, \n",
    "                table       =   TABLE_NAME_TARGET, \n",
    "                mode        =   \"append\", \n",
    "                properties  =   properties_dbspark\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
